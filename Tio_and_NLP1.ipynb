{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Succesfully imported necessary modules\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the modules and packages \n",
    "\n",
    "import os   # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import re  #  re is for regular expressions, which we use later \n",
    "from nltk import word_tokenize\n",
    "print(\"1. Succesfully imported necessary modules\")    # \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stages to pre-process a text are:\n",
    " - 1) Raw text (the data is a string)\n",
    " - 2) sentence segmentation into tokens(string is turned list of strings)\n",
    "      - 2a) some standardisation - turning to lower case and spell checking\n",
    "       - Here is where I think we'd do any other pre-processing via RegEx I think - if/as needed.\n",
    "  - 3) Part of speech tagging (we now have a list of list of tuples)\n",
    " - 4) Named Entity Recognition\n",
    "  \n",
    "It is possible wrap put several stages together, however for these purposes I have done each stage one by one.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) - start with some raw text as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = (\"She moved out last weeek. I am feeling vry sad and very lonely. I miss the little things, \\\n",
    "            like how she leaves her coffee cup in the sink and doesn't wash it up. I don't like these feelings.\")\n",
    "            #a few typos deliberate\n",
    "            #am putting lonely and sad into one sentence as that has featured in discussions\n",
    "            #deliberate use of the word \"feeling\" both as a verb and a noun\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She moved out last weeek. I am feeling vry sad and very lonely. I miss the little things,             like how she leaves her coffee cup in the sink and doesn't wash it up. I don't like these feelings.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Segmentation into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are breaking down our document into sentences. This gives us a list of strings.\n",
    "tokens = nltk.word_tokenize(document) #[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'moved',\n",
       " 'out',\n",
       " 'last',\n",
       " 'weeek',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'feeling',\n",
       " 'vry',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'very',\n",
       " 'lonely',\n",
       " '.',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'the',\n",
       " 'little',\n",
       " 'things',\n",
       " ',',\n",
       " 'like',\n",
       " 'how',\n",
       " 'she',\n",
       " 'leaves',\n",
       " 'her',\n",
       " 'coffee',\n",
       " 'cup',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sink',\n",
       " 'and',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'wash',\n",
       " 'it',\n",
       " 'up',\n",
       " '.',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'these',\n",
       " 'feelings',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens # Tokens are a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a) Some standardisation  - specifically turning to lower case and then spell checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'moved', 'out', 'last', 'weeek', '.', 'i', 'am', 'feeling', 'vry', 'sad', 'and', 'very', 'lonely', '.', 'i', 'miss', 'the', 'little', 'things', ',', 'like', 'how', 'she', 'leaves', 'her', 'coffee', 'cup', 'in', 'the', 'sink', 'and', 'does', \"n't\", 'wash', 'it', 'up', '.', 'i', 'do', \"n't\", 'like', 'these', 'feelings', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_lower = [word.lower() for word in tokens]\n",
    "print(tokens_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for a spell check - NTLK default - \n",
    "#!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "check = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'moved', 'out', 'last', 'week', '.', 'i', 'am', 'feeling', 'very', 'sad', 'and', 'very', 'lonely', '.', 'i', 'miss', 'the', 'little', 'things', ',', 'like', 'how', 'she', 'leaves']\n"
     ]
    }
   ],
   "source": [
    "correct_spell = []\n",
    "\n",
    "for word in tokens_lower:\n",
    "    correct_spell.append(check(word))    \n",
    "\n",
    "print(correct_spell[:25])\n",
    "\n",
    "#Spell checking needs attention - the default ntlk spell checker turns \"loney\" to \"money\" however works ok \n",
    "#with the errors I put in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Parts of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. This step assigns grammatical information of each word of the sentence. \n",
    "\n",
    "\n",
    "For a list of the different parts of speech see this link:\n",
    "https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 'PRP'),\n",
       " ('moved', 'VBD'),\n",
       " ('out', 'RB'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('.', '.'),\n",
       " ('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('feeling', 'VBG'),\n",
       " ('very', 'RB'),\n",
       " ('sad', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('very', 'RB'),\n",
       " ('lonely', 'RB'),\n",
       " ('.', '.'),\n",
       " ('i', 'VB'),\n",
       " ('miss', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " (',', ','),\n",
       " ('like', 'IN'),\n",
       " ('how', 'WRB'),\n",
       " ('she', 'PRP'),\n",
       " ('leaves', 'VBZ'),\n",
       " ('her', 'PRP$'),\n",
       " ('coffee', 'NN'),\n",
       " ('cup', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sink', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('does', 'VBZ'),\n",
       " (\"n't\", 'RB'),\n",
       " ('wash', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('up', 'RP'),\n",
       " ('.', '.'),\n",
       " ('i', 'NNS'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('like', 'VB'),\n",
       " ('these', 'DT'),\n",
       " ('feelings', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag \n",
    "#to each word\n",
    "tagged = nltk.pos_tag(correct_spell)\n",
    "tagged\n",
    "#Note that the first use of feeling is a verb gerund (VBG) and the second use is a plural noun (NNS). This is\n",
    "# a good thing.\n",
    "# sad is tagged as an adjective (JJ) and lonely as an adverb (RB) - what you'd expect grammatically.\n",
    "#interesting how \"i\" is not consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. I think in the context of Tio we need to improve our understanding the people in the conversations will be seperate entities.\n",
    "\n",
    "The ntlk chunk package may be able to help us. It basically helps us identify non-overlapping groups in unrestricted text. It's a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter binary=True, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  she/PRP\n",
      "  moved/VBD\n",
      "  out/RB\n",
      "  last/JJ\n",
      "  week/NN\n",
      "  ./.\n",
      "  i/NN\n",
      "  am/VBP\n",
      "  feeling/VBG\n",
      "  very/RB\n",
      "  sad/JJ\n",
      "  and/CC\n",
      "  very/RB\n",
      "  lonely/RB\n",
      "  ./.\n",
      "  i/VB\n",
      "  miss/VBP\n",
      "  the/DT\n",
      "  little/JJ\n",
      "  things/NNS\n",
      "  ,/,\n",
      "  like/IN\n",
      "  how/WRB\n",
      "  she/PRP\n",
      "  leaves/VBZ\n",
      "  her/PRP$\n",
      "  coffee/NN\n",
      "  cup/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  sink/NN\n",
      "  and/CC\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  wash/VB\n",
      "  it/PRP\n",
      "  up/RP\n",
      "  ./.\n",
      "  i/NNS\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  like/VB\n",
      "  these/DT\n",
      "  feelings/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "chunk = nltk.ne_chunk(tagged, binary = True)\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where next? Some thoughts.\n",
    " - NER needs a bit more attention - plan to go over UKDS presentation from 29 June again and look at accompanying code files. I haven't yet got to grips with chunking properly.\n",
    " - Also need to talk to S, O and M re where we are trying to get to.\n",
    " - I think we need to think about building Tio's corpus as Tio learns - need to reread chapter 11 of NTLK book?\n",
    " - Also need to think about how this relates to the architecture - in terms of how we evaluate each statement vs the cumulative conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Sources used\n",
    "See Figure 1.1: Simple Pipeline Architecture for an Information Extraction System for more detail https://www.nltk.org/book/ch07.html \n",
    "POS tagging Source for material - chapter 5 of NTLK python book on categorising text https://www.nltk.org/book/ch05.html\n",
    "\n",
    "https://ukdataservice.ac.uk/media/622724/textminingbasic16june2020.pdf\n",
    "https://ukdataservice.ac.uk/media/622739/textminingadvanced29june2020.pdf\n",
    "https://github.com/PeppermintT/Training_sessions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
