{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Tio part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    " - Classifying words into parts of speech gives us the foundation. When we have the sentence \" I am feeling very sad and lonely\", sad is tagged as an adjective (JJ) and lonely as an adverb (RB).\n",
    " \n",
    "What next:\n",
    " - I have played around with putting the parts of speech tags back into larger pieces to work out how to get some meaning from the text.\n",
    "  - Option 1: looking at chunking.\n",
    "  - Option 2: looking at linguistic structures\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Succesfully imported necessary modules\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the modules and packages \n",
    "\n",
    "import os   # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import re  #  re is for regular expressions, which we use later \n",
    "from nltk import word_tokenize\n",
    "print(\"1. Succesfully imported necessary modules\")    # \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = (\"Jane moved out last week. I am feeling very sad and very lonely. I miss the little things, \\\n",
    "            like how she leaves her coffee cup in the sink. I don't like these feelings.\")\n",
    "#I have left it as \"Jane\" rather than \"she\". Jane will be recognised as a Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jane moved out last week. I am feeling very sad and very lonely. I miss the little things,             like how she leaves her coffee cup in the sink. I don't like these feelings.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jane', 'NNP'),\n",
       " ('moved', 'VBD'),\n",
       " ('out', 'RP'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('feeling', 'VBG'),\n",
       " ('very', 'RB'),\n",
       " ('sad', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('very', 'RB'),\n",
       " ('lonely', 'RB'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('miss', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " (',', ','),\n",
       " ('like', 'IN'),\n",
       " ('how', 'WRB'),\n",
       " ('she', 'PRP'),\n",
       " ('leaves', 'VBZ'),\n",
       " ('her', 'PRP$'),\n",
       " ('coffee', 'NN'),\n",
       " ('cup', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sink', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('like', 'VB'),\n",
       " ('these', 'DT'),\n",
       " ('feelings', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn text into tokens.\n",
    "tokens = nltk.word_tokenize(document)\n",
    "#Tag the tokens\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking takes the POS segments and labels multi-token sequences. There\n",
    "are different types of chunking. I simply tried noun phrase chunking, or NP-chunking, where we search for chunks corresponding to individual noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jane', 'NNP'), ('moved', 'VBD'), ('out', 'RP'), ('last', 'JJ'), ('week', 'NN'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('feeling', 'VBG'), ('very', 'RB'), ('sad', 'JJ'), ('and', 'CC'), ('very', 'RB'), ('lonely', 'RB'), ('.', '.'), ('I', 'PRP'), ('miss', 'VBP'), ('the', 'DT'), ('little', 'JJ'), ('things', 'NNS'), (',', ','), ('like', 'IN'), ('how', 'WRB'), ('she', 'PRP'), ('leaves', 'VBZ'), ('her', 'PRP$'), ('coffee', 'NN'), ('cup', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sink', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('like', 'VB'), ('these', 'DT'), ('feelings', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine a grammar\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Jane/NNP\n",
      "  moved/VBD\n",
      "  out/RP\n",
      "  (NP last/JJ week/NN)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  feeling/VBG\n",
      "  very/RB\n",
      "  sad/JJ\n",
      "  and/CC\n",
      "  very/RB\n",
      "  lonely/RB\n",
      "  ./.\n",
      "  I/PRP\n",
      "  miss/VBP\n",
      "  the/DT\n",
      "  little/JJ\n",
      "  things/NNS\n",
      "  ,/,\n",
      "  like/IN\n",
      "  how/WRB\n",
      "  she/PRP\n",
      "  leaves/VBZ\n",
      "  her/PRP$\n",
      "  (NP coffee/NN)\n",
      "  (NP cup/NN)\n",
      "  in/IN\n",
      "  (NP the/DT sink/NN)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  like/VB\n",
      "  these/DT\n",
      "  feelings/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print (result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('feeling', 'VBG'),\n",
       " ('very', 'RB'),\n",
       " ('sad', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('very', 'RB'),\n",
       " ('lonely', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chunking1_para is very flat and too long and messy.\n",
    "#let's try just a shorter sentence.\n",
    "\n",
    "sentence = (\"I am feeling very sad and very lonely.\")\n",
    "tokens1 = nltk.word_tokenize(sentence)\n",
    "#Tag the tokens\n",
    "tagged1 = nltk.pos_tag(tokens1)\n",
    "tagged1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  feeling/VBG\n",
      "  very/RB\n",
      "  sad/JJ\n",
      "  and/CC\n",
      "  very/RB\n",
      "  lonely/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result1 = cp.parse(tagged1)\n",
    "print (result1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are two examples of chunking sentences and also drawing a tree diagram.\n",
    " - The chunking could be made more sophisticated using chunking regex rules (see ch7) - before we dive into that we would need to determine what we wanted out of it.\n",
    "  - We could also define what we want to exclude from a chunk - this is called chinking.\n",
    "  -there is a section in ch8 about evaluating chunking - not sure whether relevant or how we apply.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Option 2: Looking at linguistic structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a grammar\n",
    "from nltk import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.grammar.CFG"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        str\n",
       "\u001b[1;31mString form:\u001b[0m NP: {<DT>?<JJ>*<NN>}\n",
       "\u001b[1;31mLength:\u001b[0m      20\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "str(object='') -> str\n",
       "str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
       "\n",
       "Create a new string object from the given object. If encoding or\n",
       "errors is specified, then the object must expose a data buffer\n",
       "that will be decoded using the given encoding and error handler.\n",
       "Otherwise, returns the result of object.__str__() (if defined)\n",
       "or repr(object).\n",
       "encoding defaults to sys.getdefaultencoding().\n",
       "errors defaults to 'strict'.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grammar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'productions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-836ed42a72fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"man\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"dog\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"cat\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"telescope\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"park\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"in\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"on\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"by\"\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;34m\"with\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \"\"\")\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'productions'"
     ]
    }
   ],
   "source": [
    "grammar = nltk.grammar.CFG(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP | V NP PP\n",
    "PP -> P NP\n",
    "V -> \"saw\" | \"ate\" | \"walked\"\n",
    "NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    NP -> Det N\n",
      "    PP -> P NP\n",
      "    VP -> 'slept'\n",
      "    VP -> 'saw' NP\n",
      "    VP -> 'walked' PP\n",
      "    Det -> 'the'\n",
      "    Det -> 'a'\n",
      "    N -> 'man'\n",
      "    N -> 'park'\n",
      "    N -> 'dog'\n",
      "    P -> 'in'\n",
      "    P -> 'with'\n"
     ]
    }
   ],
   "source": [
    "#ok back to documentation, we're going to have to just find a grammar. Any grammar.\n",
    "\n",
    "from nltk.parse.generate import generate, demo_grammar\n",
    "grammar = CFG.fromstring(demo_grammar)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - we have a grammar. ish.\n",
    "NTLK defines grammar as: \n",
    " - A grammar is a compact characterization of a potentially infinite set of sentences; we say that a tree is well-formed according to a grammar, or that a grammar licenses a tree.\n",
    " - A grammar is a formal model for describing whether a given phrase can be assigned a particular constituent or dependency structure.\n",
    " \n",
    " .....will need to come back to this.....\n",
    " \n",
    " ....also I am not sure how chunking relates to grammars. How do the two concepts relate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ShiftReduceParser.parse at 0x000002A5039AC780>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_parse = nltk.ShiftReduceParser(grammar)\n",
    "sent = 'I am feeling very sad and very lonely'.split()\n",
    "result2 = sr_parse.parse(sent)\n",
    "result2\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.nltk.org/howto/generate.html\n",
    "#http://www.nltk.org/book_1ed/ch08.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n"
     ]
    }
   ],
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "print (tree1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP the rabbit)\n"
     ]
    }
   ],
   "source": [
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "print (tree2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other\n",
    "# i think the subclauses Sanjay was talking about feature in 9.3 - Extending a Feature based Grammar........"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
