{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Text Corpus\n",
    "A text corpus is simply a large body of text. The nltk gutenberg corpus contains a small portion of texts that I am now working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words ('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34110"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a short program to display other information about each text, by looping over all the \n",
    "values of fileid corresponding to the gutenberg file identifiers listed earlier and then \n",
    "computing statistics for each text. \n",
    "\n",
    "Start with looping over each file and summarise number of characters,words, sentences and vocab\n",
    "The calculate three statistics for each text: average word length, average sentence length, and the number of times each vocabulary item appears in the text on average (our lexical diversity score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "#gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#_characters 887071 #_words 192427 #_sentences 7752 #_vocab 7344 austen-emma.txt\n",
      "5 25 26 austen-emma.txt\n",
      "None\n",
      "#_characters 466292 #_words 98171 #_sentences 3747 #_vocab 5835 austen-persuasion.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "None\n",
      "#_characters 673022 #_words 141576 #_sentences 4999 #_vocab 6403 austen-sense.txt\n",
      "5 28 22 austen-sense.txt\n",
      "None\n",
      "#_characters 4332554 #_words 1010654 #_sentences 30103 #_vocab 12767 bible-kjv.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "None\n",
      "#_characters 38153 #_words 8354 #_sentences 438 #_vocab 1535 blake-poems.txt\n",
      "5 19 5 blake-poems.txt\n",
      "None\n",
      "#_characters 249439 #_words 55563 #_sentences 2863 #_vocab 3940 bryant-stories.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "None\n",
      "#_characters 84663 #_words 18963 #_sentences 1054 #_vocab 1559 burgess-busterbrown.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "None\n",
      "#_characters 144395 #_words 34110 #_sentences 1703 #_vocab 2636 carroll-alice.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "None\n",
      "#_characters 457450 #_words 96996 #_sentences 4779 #_vocab 8335 chesterton-ball.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "None\n",
      "#_characters 406629 #_words 86063 #_sentences 3806 #_vocab 7794 chesterton-brown.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "None\n",
      "#_characters 320525 #_words 69213 #_sentences 3742 #_vocab 6349 chesterton-thursday.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "None\n",
      "#_characters 935158 #_words 210663 #_sentences 10230 #_vocab 8447 edgeworth-parents.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "None\n",
      "#_characters 1242990 #_words 260819 #_sentences 10059 #_vocab 17231 melville-moby_dick.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "None\n",
      "#_characters 468220 #_words 96825 #_sentences 1851 #_vocab 9021 milton-paradise.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "None\n",
      "#_characters 112310 #_words 25833 #_sentences 2163 #_vocab 3032 shakespeare-caesar.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "None\n",
      "#_characters 162881 #_words 37360 #_sentences 3106 #_vocab 4716 shakespeare-hamlet.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "None\n",
      "#_characters 100351 #_words 23140 #_sentences 1907 #_vocab 3464 shakespeare-macbeth.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "None\n",
      "#_characters 711215 #_words 154883 #_sentences 4250 #_vocab 12452 whitman-leaves.txt\n",
      "5 36 12 whitman-leaves.txt\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))# number of characters in each file; includes spaces\n",
    "    num_words = len(gutenberg.words(fileid))# number of words in each file\n",
    "    num_sents = len(gutenberg.sents(fileid))#number of sentences in each file\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))# unique words - a set\n",
    "    print(\"#_characters\", round(num_chars), \"#_words\" , round (num_words),\n",
    "          \"#_sentences\", round(num_sents), \"#_vocab\", round(num_vocab), fileid)\n",
    "    #and now print the three stats\n",
    "    print(print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average word length appears to be a general property of English, since it has a recurrent value of 4. (In fact, the average word length is really 3 not 4, since the num_chars variable counts space characters.) \n",
    "By contrast average sentence length and lexical diversity appear to be characteristics of particular authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thunder', 'and', 'Lightning', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')# sents function \n",
    "#splits text into a list of sentences\n",
    "macbeth_sentences[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Double',\n",
       " ',',\n",
       " 'double',\n",
       " ',',\n",
       " 'toile',\n",
       " 'and',\n",
       " 'trouble',\n",
       " ';',\n",
       " 'Fire',\n",
       " 'burne',\n",
       " ',',\n",
       " 'and',\n",
       " 'Cauldron',\n",
       " 'bubble']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences[1116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "longest_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doubtfull',\n",
       "  'it',\n",
       "  'stood',\n",
       "  ',',\n",
       "  'As',\n",
       "  'two',\n",
       "  'spent',\n",
       "  'Swimmers',\n",
       "  ',',\n",
       "  'that',\n",
       "  'doe',\n",
       "  'cling',\n",
       "  'together',\n",
       "  ',',\n",
       "  'And',\n",
       "  'choake',\n",
       "  'their',\n",
       "  'Art',\n",
       "  ':',\n",
       "  'The',\n",
       "  'mercilesse',\n",
       "  'Macdonwald',\n",
       "  '(',\n",
       "  'Worthie',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'Rebell',\n",
       "  ',',\n",
       "  'for',\n",
       "  'to',\n",
       "  'that',\n",
       "  'The',\n",
       "  'multiplying',\n",
       "  'Villanies',\n",
       "  'of',\n",
       "  'Nature',\n",
       "  'Doe',\n",
       "  'swarme',\n",
       "  'vpon',\n",
       "  'him',\n",
       "  ')',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Westerne',\n",
       "  'Isles',\n",
       "  'Of',\n",
       "  'Kernes',\n",
       "  'and',\n",
       "  'Gallowgrosses',\n",
       "  'is',\n",
       "  'supply',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  ',',\n",
       "  'And',\n",
       "  'Fortune',\n",
       "  'on',\n",
       "  'his',\n",
       "  'damned',\n",
       "  'Quarry',\n",
       "  'smiling',\n",
       "  ',',\n",
       "  'Shew',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'like',\n",
       "  'a',\n",
       "  'Rebells',\n",
       "  'Whore',\n",
       "  ':',\n",
       "  'but',\n",
       "  'all',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'too',\n",
       "  'weake',\n",
       "  ':',\n",
       "  'For',\n",
       "  'braue',\n",
       "  'Macbeth',\n",
       "  '(',\n",
       "  'well',\n",
       "  'hee',\n",
       "  'deserues',\n",
       "  'that',\n",
       "  'Name',\n",
       "  ')',\n",
       "  'Disdayning',\n",
       "  'Fortune',\n",
       "  ',',\n",
       "  'with',\n",
       "  'his',\n",
       "  'brandisht',\n",
       "  'Steele',\n",
       "  ',',\n",
       "  'Which',\n",
       "  'smoak',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'with',\n",
       "  'bloody',\n",
       "  'execution',\n",
       "  '(',\n",
       "  'Like',\n",
       "  'Valours',\n",
       "  'Minion',\n",
       "  ')',\n",
       "  'caru',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'out',\n",
       "  'his',\n",
       "  'passage',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'hee',\n",
       "  'fac',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'the',\n",
       "  'Slaue',\n",
       "  ':',\n",
       "  'Which',\n",
       "  'neu',\n",
       "  \"'\",\n",
       "  'r',\n",
       "  'shooke',\n",
       "  'hands',\n",
       "  ',',\n",
       "  'nor',\n",
       "  'bad',\n",
       "  'farwell',\n",
       "  'to',\n",
       "  'him',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'he',\n",
       "  'vnseam',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'him',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Naue',\n",
       "  'toth',\n",
       "  \"'\",\n",
       "  'Chops',\n",
       "  ',',\n",
       "  'And',\n",
       "  'fix',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'his',\n",
       "  'Head',\n",
       "  'vpon',\n",
       "  'our',\n",
       "  'Battlements']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in macbeth_sentences if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.fileids()) # we have 500 files.....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in 15 categories\n",
    "len(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01',\n",
       " 'ca02',\n",
       " 'ca03',\n",
       " 'ca04',\n",
       " 'ca05',\n",
       " 'ca06',\n",
       " 'ca07',\n",
       " 'ca08',\n",
       " 'ca09',\n",
       " 'ca10',\n",
       " 'ca11',\n",
       " 'ca12',\n",
       " 'ca13',\n",
       " 'ca14',\n",
       " 'ca15',\n",
       " 'ca16',\n",
       " 'ca17',\n",
       " 'ca18',\n",
       " 'ca19',\n",
       " 'ca20',\n",
       " 'ca21',\n",
       " 'ca22',\n",
       " 'ca23',\n",
       " 'ca24',\n",
       " 'ca25',\n",
       " 'ca26',\n",
       " 'ca27',\n",
       " 'ca28',\n",
       " 'ca29',\n",
       " 'ca30',\n",
       " 'ca31',\n",
       " 'ca32',\n",
       " 'ca33',\n",
       " 'ca34',\n",
       " 'ca35',\n",
       " 'ca36',\n",
       " 'ca37',\n",
       " 'ca38',\n",
       " 'ca39',\n",
       " 'ca40',\n",
       " 'ca41',\n",
       " 'ca42',\n",
       " 'ca43',\n",
       " 'ca44']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.fileids(categories='news') # the file ids aren't very informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')# end puts output on single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6386, ',': 5188, '.': 4030, 'of': 2861, 'and': 2186, 'to': 2144, 'a': 2130, 'in': 2020, 'for': 969, 'that': 829, ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to obtain counts for each genre of interest. We'll use NLTK's support for conditional frequency distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most frequent modal in the news genre is will, \n",
    "#while the most frequent modal in the romance genre is could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
